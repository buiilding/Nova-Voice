services:
  # Redis for queuing and pub/sub
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - speech_network

  # Gateway service - handles WebSocket connections
  gateway:
    build:
      context: ../gateway
      dockerfile: Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: 1
    ports:
      - "${GATEWAY_PORT}:${GATEWAY_PORT}"  # WebSocket port
      - "${HEALTH_PORT}:${HEALTH_PORT}"  # Health port
    environment:
      - REDIS_URL=${REDIS_URL}
      - GATEWAY_PORT=${GATEWAY_PORT}
      - HEALTH_PORT=${HEALTH_PORT}
      - SILENCE_THRESHOLD_SECONDS=${SILENCE_THRESHOLD_SECONDS}
      - SAMPLE_RATE=${SAMPLE_RATE}
      - BUFFER_SIZE=${BUFFER_SIZE}
      - WEBRTC_SENSITIVITY=${WEBRTC_SENSITIVITY}
      - SILERO_SENSITIVITY=${SILERO_SENSITIVITY}
      - PRE_SPEECH_BUFFER_SECONDS=${PRE_SPEECH_BUFFER_SECONDS}
      - MAX_QUEUE_DEPTH=${MAX_QUEUE_DEPTH}
      - LOG_LEVEL=${LOG_LEVEL}
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - speech_network
    deploy:
      replicas: 1  # Scale this for multiple gateway instances
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${HEALTH_PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # STT Worker service - speech-to-text transcription
  stt_worker:
    build:
      context: ../stt_worker
      dockerfile: Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: 1
    environment:
      - REDIS_URL=${REDIS_URL}
      # WORKER_ID intentionally omitted (auto-generated)
      - MODEL_SIZE=${MODEL_SIZE}
      - COMPUTE_TYPE=${COMPUTE_TYPE}
      - DEVICE=${DEVICE}
      - BEAM_SIZE=${BEAM_SIZE}
      - INITIAL_PROMPT=${INITIAL_PROMPT}
      - NORMALIZE_AUDIO=${NORMALIZE_AUDIO}
      - PENDING_ACK_TTL=${PENDING_ACK_TTL}
      - HEALTH_PORT=${HEALTH_PORT}
      - VAD_FILTER=${VAD_FILTER}
      - LOG_LEVEL=${LOG_LEVEL}
      - NVIDIA_VISIBLE_DEVICES=all
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - speech_network
    deploy:
      replicas: 1  # Scale this: docker-compose up --scale stt_worker=3
    # GPU support - uncomment and use with nvidia runtime
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${HEALTH_PORT}/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s  # Allow time for model loading

  # Translation Worker service - text translation
  translation_worker:
    build:
      context: ../translation_worker
      dockerfile: Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: 1
    environment:
      - REDIS_URL=${REDIS_URL}
      # WORKER_ID intentionally omitted (auto-generated)
      - HEALTH_PORT=${HEALTH_PORT}
      - LOG_LEVEL=${LOG_LEVEL}
      - NVIDIA_VISIBLE_DEVICES=all
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - speech_network
    deploy:
      replicas: 1  # Scale this: docker-compose up --scale translation_worker=2
    # GPU support - uncomment and use with nvidia runtime
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${HEALTH_PORT}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  speech_network:
    driver: bridge

volumes:
  redis_data:
    driver: local

# Example scaling commands (run after docker-compose up):
# docker-compose up --scale stt_worker=3 --scale translation_worker=2 --scale gateway=2
#
# For GPU support, add to docker-compose.override.yml:
# version: '3.8'
# services:
#   stt_worker:
#     runtime: nvidia
#     environment:
#       - NVIDIA_VISIBLE_DEVICES=all
#       - DEVICE=cuda
#
# Or run with GPU support:
# docker-compose up --scale stt_worker=2
# docker run --gpus all --rm -it <stt_worker_image>

